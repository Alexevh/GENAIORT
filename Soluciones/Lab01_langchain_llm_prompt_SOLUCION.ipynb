{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBCSNaCdrBz-"
   },
   "source": [
    "# Laboratorio: Setup y uso b√°sico de LLMs con LangChain + Prompt engineering avanzado\n",
    "\n",
    "Este laboratorio est√° pensado para completarse en ~2 horas. Trabajaremos en Google Colab con recursos gratuitos, usando la Inference API de Hugging Face y un modelo instruct abierto.\n",
    "\n",
    "- Contenidos:\n",
    "  - Setup en Colab y configuraci√≥n de Hugging Face Inference API\n",
    "  - Uso b√°sico de LLMs con LangChain (LCEL)\n",
    "  - Par√°metros de decodificaci√≥n y control de estilo\n",
    "  - Prompt engineering avanzado: zero-shot, few-shot, Chain of Thought y salida estructurada (JSON)\n",
    "\n",
    "Al finalizar, podr√°s:\n",
    "- Conectarte a un LLM instruct v√≠a Hugging Face Inference API desde LangChain.\n",
    "- Construir cadenas simples con `prompt | llm | parser`.\n",
    "- Dise√±ar prompts efectivos y controlar formato de salida (incluido JSON).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RTG7_ybrBz_"
   },
   "source": [
    "## Parte 0 ‚Äî Setup (Colab + librer√≠as + token)\n",
    "\n",
    "Usaremos versiones estables para minimizar fricci√≥n en Colab. Asegurate de ejecutar esta secci√≥n antes de continuar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29325,
     "status": "ok",
     "timestamp": 1756393838666,
     "user": {
      "displayName": "Joaquin Bonifacino",
      "userId": "00057501700847538249"
     },
     "user_tz": 180
    },
    "id": "cxoJSkLcrBz_",
    "outputId": "7440883d-b45f-486f-e9f6-9bc9e47d9a42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Instalar dependencias principales (versiones estables)\n",
    "!pip -q install -U \\\n",
    "  \"langchain==0.3.27\" \\\n",
    "  \"langchain-community==0.3.27\" \\\n",
    "  \"langchain-huggingface==0.3.1\" \\\n",
    "  \"transformers==4.55.2\" \\\n",
    "  \"huggingface_hub==0.34.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1756393838910,
     "user": {
      "displayName": "Joaquin Bonifacino",
      "userId": "00057501700847538249"
     },
     "user_tz": 180
    },
    "id": "OmVbT5uRxn5R",
    "outputId": "7476bd33-3811-4185-e06f-e9f7836944da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain => 0.3.27\n",
      "langchain-community => 0.3.27\n",
      "langchain-huggingface => 0.3.1\n",
      "transformers => 4.55.2\n",
      "huggingface_hub => 0.34.4\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version, PackageNotFoundError\n",
    "\n",
    "for dist in [\"langchain\", \"langchain-community\", \"langchain-huggingface\",\n",
    "             \"transformers\", \"huggingface_hub\"]:\n",
    "    try:\n",
    "        print(dist, \"=>\", version(dist))\n",
    "    except PackageNotFoundError:\n",
    "        print(dist, \"no instalado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7955,
     "status": "ok",
     "timestamp": 1756393846866,
     "user": {
      "displayName": "Joaquin Bonifacino",
      "userId": "00057501700847538249"
     },
     "user_tz": 180
    },
    "id": "M9EJxPOCrB0A",
    "outputId": "50248f2c-21c8-431c-fdae-4d7d951efa16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
      "PyTorch: 2.8.0+cu126\n",
      "CUDA disponible: False\n"
     ]
    }
   ],
   "source": [
    "# Comprobar versi√≥n de Python y GPU/CPU\n",
    "import sys, subprocess, torch\n",
    "print(sys.version)\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "except Exception as e:\n",
    "    print(\"PyTorch no disponible o sin CUDA\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ug4tn-VrB0A"
   },
   "source": [
    "### Configuraci√≥n del token de Hugging Face\n",
    "\n",
    "Para usar la Hugging Face Inference API, necesit√°s un token personal (gratuito). En Colab se recomienda guardarlo en `userdata`:\n",
    "\n",
    "1. Crear el token en `https://huggingface.co/settings/tokens`.\n",
    "2. En Colab: Abre el menu en la barra izquierda haciendo click en la llave ‚Üí \"Agregar nuevo secreto\" ponle `HF_TOKEN` y el token que generamos ‚Üí Abilita el acceso al notebook.\n",
    "3. Ejecutar la celda siguiente para leerlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1756393847450,
     "user": {
      "displayName": "Joaquin Bonifacino",
      "userId": "00057501700847538249"
     },
     "user_tz": 180
    },
    "id": "DMTYDzh9rB0A",
    "outputId": "e3816a08-48ac-4f2a-848b-fcfbc6f39ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token cargado OK: {HF_TOKEN}\n"
     ]
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "assert HF_TOKEN is not None and len(HF_TOKEN) > 0, \"Configurar el secreto 'HF_TOKEN' en Colab.\"\n",
    "print(\"Token cargado OK: {HF_TOKEN}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQbt6t3FrB0A"
   },
   "source": [
    "## Parte 1 ‚Äî Uso b√°sico de LLMs con LangChain\n",
    "\n",
    "Trabajaremos con un modelo instruct accesible v√≠a Inference API. Para minimizar fricci√≥n, usaremos un modelo abierto y, cuando sea posible, una variante quantizada (el ya hosteado en HF).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1458,
     "status": "ok",
     "timestamp": 1756393944261,
     "user": {
      "displayName": "Joaquin Bonifacino",
      "userId": "00057501700847538249"
     },
     "user_tz": 180
    },
    "id": "fNzk__gZrB0B",
    "outputId": "abb8b0bf-ab04-4a36-adda-a863f3c4ffd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un LLM (Large Language Model) es un tipo de modelo de inteligencia artificial entrenado en grandes vol√∫menes de texto para entender, generar y realizar tareas relacionadas con el lenguaje humano.  \n",
      "Casos de uso incluyen la redacci√≥n de contenido automatizada y asistencia en resoluci√≥n de preguntas.  \n",
      "Estos modelos tambi√©n se utilizan en chatbots, traducci√≥n autom√°tica y generaci√≥n de c√≥digo.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "hf_endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=MODEL_ID,\n",
    "    task=\"conversational\",\n",
    "    huggingfacehub_api_token=\"HF_TOKEN\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "llm = ChatHuggingFace(llm=hf_endpoint)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente √∫til y conciso.\"),\n",
    "    (\"human\", \"Responde a la siguiente instrucci√≥n: {instruccion}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\"instruccion\": \"Explica en 3 frases qu√© es un LLM y nombra 2 casos de uso.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM0fs9ZLrB0B"
   },
   "source": [
    "### Ejercicio 1.1 (10 min)\n",
    "\n",
    "- Probar 3 variaciones de `temperature` y observar el cambio en estilo.\n",
    "- Cambiar el rol del `system` para forzar un estilo (p.ej., ‚Äúresponde con vi√±etas y m√°ximo 3 l√≠neas‚Äù).\n",
    "- Pregunta sugerida: ‚ÄúResume la diferencia entre entrenamiento y fine-tuning en 3 puntos.‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mcTYMM-srB0B",
    "outputId": "22adbb5a-1be3-4655-caec-b27466e2e669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== temperature: 0.0 ====\n",
      " 1. **Entrenamiento (training)**: Se refiere al proceso inicial de entrenar un modelo desde cero o a partir de un modelo preentrenado en grandes vol√∫menes de datos generales, para que aprenda caracter√≠sticas b√°sicas de un dominio o tarea espec√≠fica.\n",
      "\n",
      "2. **Fine-tuning**: Consiste en ajustar un modelo ya entrenado (normalmente en un dominio general) con datos m√°s espec√≠ficos y relevantes para una tarea o dominio particular, para mejorar su rendimiento en esa tarea espec√≠fica.\n",
      "\n",
      "3. **Objetivo**: El entrenamiento busca aprender representaciones generales, mientras que el fine-tuning busca optimizar el modelo para una aplicaci√≥n concreta, usando menos recursos y tiempo que un entrenamiento desde cero.\n",
      "\n",
      "==== temperature: 0.7 ====\n",
      " 1. **Entrenamiento (training)**: Consiste en entrenar un modelo desde cero o a partir de un modelo inicial, utilizando un conjunto de datos amplio y diverso para aprender patrones generales de una tarea o dominio.\n",
      "\n",
      "2. **Fine-tuning**: Se realiza despu√©s de un entrenamiento inicial, utilizando un conjunto de datos m√°s espec√≠fico para ajustar el modelo a una tarea o domino particulares, lo que mejora su rendimiento en ese contexto.\n",
      "\n",
      "3. **Objetivo**: El entrenamiento busca aprender habilidades generales, mientras que el fine-tuning busca optimizar el modelo para una aplicaci√≥n espec√≠fica, haciendo que sea m√°s preciso y adecuado.\n",
      "\n",
      "==== temperature: 1.2 ====\n",
      " 1. **Entrenamiento (training)**: Se realiza desde cero o con un modelo inicial, usando un conjunto de datos amplio y diverso para aprender patrones generales de datos, como lenguaje, im√°genes o n√∫meros.  \n",
      "\n",
      "2. **Fine-tuning**: Se aplica despu√©s de un entrenamiento inicial, ajustando el modelo con un conjunto de datos m√°s espec√≠fico y peque√±o para adaptarlo a una tarea o dominio particular (por ejemplo, clasificaci√≥n de noticias o traducci√≥n).  \n",
      "\n",
      "3. **Objetivo**: El entrenamiento busca aprender representaciones generales; el fine-tuning busca optimizar el modelo para una tarea espec√≠fica, mejorando su precisi√≥n y rendimiento en entornos reales.\n",
      "\n",
      "==== estilo forzado ====:\n",
      " - Entrenamiento: se entrena un modelo desde cero o a partir de un modelo inicial con datos gen√©ricos.  \n",
      "- Fine-tuning: se adapta un modelo ya entrenado a un dominio espec√≠fico usando datos relevantes.  \n",
      "- Fine-tuning requiere menos recursos que el entrenamiento desde cero y es m√°s eficiente para tareas espec√≠ficas.\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 1.1 ‚Äî Variaciones de temperatura y estilo (compatible con nscale)\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "instruccion: str = \"Resume la diferencia entre entrenamiento y fine-tuning en 3 puntos.\"\n",
    "\n",
    "# Variar temperatura (crea un endpoint conversacional por cada T)\n",
    "for t in [0.0, 0.7, 1.2]:\n",
    "    tmp_endpoint = HuggingFaceEndpoint(\n",
    "        repo_id=MODEL_ID,\n",
    "        task=\"conversational\",\n",
    "        huggingfacehub_api_token=\"HF_TOKEN\",\n",
    "        temperature=t,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "    tmp_llm = ChatHuggingFace(llm=tmp_endpoint)\n",
    "    respuesta = (prompt | tmp_llm | StrOutputParser()).invoke({\"instruccion\": instruccion})\n",
    "    print(\"\\n==== temperature:\", t, \"====\\n\", respuesta)\n",
    "\n",
    "# Forzar estilo con system prompt (usa un LLM base conversacional)\n",
    "base_endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=MODEL_ID,\n",
    "    task=\"conversational\",\n",
    "    huggingfacehub_api_token=HF_TOKEN,\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "llm_base = ChatHuggingFace(llm=base_endpoint)\n",
    "\n",
    "prompt_estilo = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres conciso. Responde SIEMPRE con vi√±etas y m√°ximo 3 l√≠neas.\"),\n",
    "    (\"human\", \"{instruccion}\"),\n",
    "])\n",
    "\n",
    "respuesta_estilo = (prompt_estilo | llm_base | StrOutputParser()).invoke({\"instruccion\": instruccion})\n",
    "print(\"\\n==== estilo forzado ====:\\n\", respuesta_estilo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mi_Cg8IMrB0C"
   },
   "source": [
    "## Parte 1.2 ‚Äî Par√°metros de decodificaci√≥n\n",
    "\n",
    "Ajustaremos par√°metros como `top_p`, `repetition_penalty` y `max_new_tokens` para observar su efecto en la generaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6lCnXq_QrB0C",
    "outputId": "f1c41891-4547-4ab4-ea39-d780948d4125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== baseline ({'temperature': 0.7, 'top_p': 0.95, 'repetition_penalty': 1.0, 'max_new_tokens': 128}) ====\n",
      "¬°Claro! Aqu√≠ tienes una analog√≠a breve y sencilla:\n",
      "\n",
      "**RAG es como si tuvieras un libro enorme con informaci√≥n actualizada, y en lugar de memorizarlo todo, cada vez que necesitas una respuesta, buscas solo los p√°rrafos m√°s relevantes que te ayuden a responder con algo √∫til y preciso.**\n",
      "\n",
      "As√≠, no se trata de recordar todo, sino de buscar la informaci√≥n necesaria en tiempo real.\n",
      "\n",
      "==== creativo ({'temperature': 1.1, 'top_p': 0.9, 'repetition_penalty': 1.0, 'max_new_tokens': 128}) ====\n",
      "Imagina que tienes un gran libro con millones de p√°ginas, y necesitas responder una pregunta. En lugar de leer todo el libro, solo buscas las p√°ginas m√°s relevantes que tengan informaci√≥n sobre tu pregunta. El RAG (Retrieval-Augmented Generation) es como un sistema que busca r√°pidamente las p√°ginas m√°s √∫tiles en un libro enorme (los datos) y luego las usa para crear una respuesta clara y precisa. As√≠, se combina inteligencia del conocimiento existente con la capacidad de generar respuestas.\n",
      "\n",
      "==== controlado ({'temperature': 0.2, 'top_p': 0.8, 'repetition_penalty': 1.1, 'max_new_tokens': 96}) ====\n",
      "Imagina que tienes un gran libro de recetas, y quieres cocinar un plato que no has visto nunca. En lugar de abrir el libro y buscar directamente la receta, decides preguntarle a un chef que conoce bien los ingredientes. √âl te responde con informaci√≥n directa, basada en lo que ha aprendido y en lo que sabes. As√≠, RAG (Retrieval-Augmented Generation) es como ese chef: no inventa la receta, sino que busca informaci√≥n √∫til en una gran cantidad de datos para ayudar a crear una respuesta precisa y relevante.\n"
     ]
    }
   ],
   "source": [
    "# Exploraci√≥n de par√°metros de decodificaci√≥n\n",
    "from typing import Dict, Any\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "consulta: str = \"Escribe una analog√≠a breve para explicar RAG a un p√∫blico no t√©cnico.\"\n",
    "\n",
    "configuraciones: Dict[str, Dict[str, Any]] = {\n",
    "    \"baseline\":   {\"temperature\": 0.7, \"top_p\": 0.95, \"repetition_penalty\": 1.0, \"max_new_tokens\": 128},\n",
    "    \"creativo\":   {\"temperature\": 1.1, \"top_p\": 0.90, \"repetition_penalty\": 1.0, \"max_new_tokens\": 128},\n",
    "    \"controlado\": {\"temperature\": 0.2, \"top_p\": 0.80, \"repetition_penalty\": 1.1, \"max_new_tokens\": 96},\n",
    "}\n",
    "\n",
    "for nombre, cfg in configuraciones.items():\n",
    "    tmp_endpoint = HuggingFaceEndpoint(\n",
    "        repo_id=MODEL_ID,\n",
    "        task=\"conversational\",\n",
    "        huggingfacehub_api_token=HF_TOKEN,\n",
    "        temperature=cfg[\"temperature\"],\n",
    "        top_p=cfg[\"top_p\"],\n",
    "        repetition_penalty=cfg[\"repetition_penalty\"],\n",
    "        max_new_tokens=cfg[\"max_new_tokens\"],\n",
    "    )\n",
    "    tmp_llm = ChatHuggingFace(llm=tmp_endpoint)\n",
    "    salida = (prompt | tmp_llm | StrOutputParser()).invoke({\"instruccion\": consulta})\n",
    "    print(f\"\\n==== {nombre} ({cfg}) ====\\n{salida}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFWQ7vryrB0C"
   },
   "source": [
    "## Parte 2 ‚Äî Prompt engineering avanzado\n",
    "\n",
    "Exploraremos estrategias para mejorar la calidad y control de las respuestas: zero-shot, few-shot, restricciones de estilo, salida estructurada, Chain of Thought y Self-Consistency/Ensemble.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1U5MiS1rB0C"
   },
   "source": [
    "### Conceptos clave: Zero-shot, Few-shot, CoT, Roles y Self-Consistency\n",
    "\n",
    "- Zero-shot: el modelo resuelve la tarea solo con instrucciones; no se proporcionan ejemplos.\n",
    "- Few-shot: adem√°s de la instrucci√≥n, se incluyen 1-3 ejemplos que muestran el formato y estilo deseados.\n",
    "- Chain-of-Thought (CoT): se gu√≠a al modelo para mostrar pasos intermedios de razonamiento (p.ej., ‚Äúrazona paso a paso‚Äù) antes de una respuesta final.\n",
    "- Role prompting: se asigna un rol (p.ej., ‚Äúact√∫a como profesor de IA‚Äù) para influir en el estilo y nivel de detalle.\n",
    "- Self-Consistency / Ensemble: se generan m√∫ltiples respuestas estoc√°sticas (con temperatura > 0) y se vota/selecciona la respuesta m√°s consistente o frecuente.\n",
    "- JSON output: se pide una salida estricta en formato JSON y se valida con un parser.\n",
    "\n",
    "Lectura recomendada: gu√≠a de prompt engineering avanzada en `https://learnprompting.org/docs/introduction`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiF9jJRfrB0C"
   },
   "source": [
    "### Zero-shot y Few-shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fNZ9KnrLrB0D",
    "outputId": "0d87f5fd-e84b-4da8-d926-d12e9494575f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ZERO-SHOT ===\n",
      "Overfitting occurs when a model learns noise instead of patterns.  \n",
      "Overcomplicated models fail on new, unseen data.  \n",
      "Overfitting reduces generalization in predictions.\n",
      "\n",
      "=== FEW-SHOT ===\n",
      "O Viola la generalizaci√≥n al ajustarse demasiado a datos de entrenamiento  \n",
      "V Eval√∫a mal en datos nuevos por falta de robustez  \n",
      "E Excede el poder de representaci√≥n del modelo\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot vs Few-shot (diferencia marcada con patr√≥n acr√≥stico)\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM mas determinista\n",
    "det_endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=MODEL_ID, task=\"conversational\",\n",
    "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=128\n",
    ")\n",
    "llm_det = ChatHuggingFace(llm=det_endpoint)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Patr√≥n no obvio: acr√≥stico O-V-E (Overfitting), cada l√≠nea empieza con esa letra\n",
    "instruccion = (\n",
    "    \"Escribe EXACTAMENTE 3 l√≠neas sobre 'overfitting'. \"\n",
    "    \"Cada l√≠nea debe comenzar con O, luego V, luego E (en ese orden). \"\n",
    "    \"6-10 palabras por l√≠nea. Sin texto extra.\"\n",
    ")\n",
    "\n",
    "zero_shot = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Sigue estrictamente las instrucciones del usuario.\"),\n",
    "    (\"human\", \"{instruccion}\")\n",
    "])\n",
    "\n",
    "few_shot = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Sigue estrictamente las instrucciones del usuario.\"),\n",
    "    # Ejemplo: el patr√≥n se demuestra con otro tema y otro acr√≥stico (R-E-G)\n",
    "    (\"human\", \"Escribe EXACTAMENTE 3 l√≠neas sobre 'regularizaci√≥n'. \"\n",
    "              \"Cada l√≠nea debe comenzar con R, luego E, luego G. \"\n",
    "              \"6-10 palabras por l√≠nea. Sin texto extra.\"),\n",
    "    (\"ai\", \"R Reduce complejidad para evitar ajustes al ruido\\n\"\n",
    "           \"E Estabiliza el aprendizaje con penalizaciones adecuadas\\n\"\n",
    "           \"G Generaliza mejor limitando pesos excesivamente grandes\"),\n",
    "    # Ahora se pide el caso real con el acr√≥stico O-V-E\n",
    "    (\"human\", \"{instruccion}\")\n",
    "])\n",
    "\n",
    "print(\"=== ZERO-SHOT ===\")\n",
    "print((zero_shot | llm_det | parser).invoke({\"instruccion\": instruccion}))\n",
    "\n",
    "print(\"\\n=== FEW-SHOT ===\")\n",
    "print((few_shot | llm_det | parser).invoke({\"instruccion\": instruccion}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b7hTF4wrB0D"
   },
   "source": [
    "### Role prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_VhEgQZrB0D",
    "outputId": "fd5e846d-cafc-4dff-fb4f-f762db9e9cd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, aqu√≠ tienes una explicaci√≥n breve y clara:\n",
      "\n",
      "**Qu√© es el aprendizaje por refuerzo (Reinforcement Learning):**  \n",
      "Es un tipo de inteligencia artificial donde un agente (como un robot o un programa) aprende a tomar decisiones para maximizar una recompensa a lo largo del tiempo. No tiene acceso a datos etiquetados, sino que aprende probando diferentes acciones y vi√©ndose recompensado o sancionado seg√∫n los resultados.\n",
      "\n",
      "üîç **Por ejemplo:**  \n",
      "Imagina que un agente quiere aprender a jugar al ajedrez. Cada vez que gana, recibe una recompensa. Si pierde, la recompensa es baja o negativa. Con el tiempo, el agente aprende qu√© movimientos le llevan a ganar m√°s.\n",
      "\n",
      "### 2 ejemplos de aplicaci√≥n:\n",
      "1. **Juegos de video** (como AlphaGo o Deep Q-Networks): Los algoritmos aprenden a jugar mejor jugando miles de partidas y recibiendo recompensas por ganar.\n",
      "2. **Rob√≥tica y automatizaci√≥n** (como un robot que aprende a caminar o navegar): El robot recibe recompensas cuando se mueve bien o evita ca√≠das, y ajusta su comportamiento para mejorar.\n",
      "\n",
      "En resumen: **El aprendizaje por refuerzo es aprender por intentos, errores y recompensas.**\n"
     ]
    }
   ],
   "source": [
    "# Role prompting\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "role_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Act√∫a como profesor de IA de nivel intermedio. S√© claro, estructurado y usa ejemplos sencillos.\"),\n",
    "    (\"human\", \"Explica brevemente qu√© es el aprendizaje por refuerzo y menciona 2 ejemplos de aplicaci√≥n.\"),\n",
    "])\n",
    "\n",
    "print((role_prompt | llm | StrOutputParser()).invoke({}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFhoZDI4rB0D"
   },
   "source": [
    "### CoT (Chain-of-Thought) prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AK2aANwrB0D",
    "outputId": "2e40d0fd-99ee-4a28-943a-63dc34811eeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIN CoT ===\n",
      "9.09%\n",
      "\n",
      "=== CON CoT ===\n",
      "Vamos a resolver este problema paso a paso usando el **teorema de Bayes**.\n",
      "\n",
      "---\n",
      "\n",
      "### Datos del problema:\n",
      "\n",
      "- Probabilidad de tener la enfermedad (prevalencia):  \n",
      "  \\( P(E) = 1\\% = 0.01 \\)\n",
      "\n",
      "- Probabilidad de no tener la enfermedad:  \n",
      "  \\( P(\\neg E) = 1 - 0.01 = 0.99 \\)\n",
      "\n",
      "- Sensibilidad de la prueba (probabilidad de dar positivo si tienes la enfermedad):  \n",
      "  \\( P(T^+ | E) = 90\\% = 0.90 \\)\n",
      "\n",
      "- Especificidad de la prueba (probabilidad de dar negativo si no tienes la enfermedad):  \n",
      "  \\( P(T^- | \\neg E) = 90\\% = 0.90 \\)\n",
      "\n",
      "Esto implica que:\n",
      "\n",
      "- El error de falsa positiva es \\( P(T^+ | \\neg E) = 1 - 0.90 = 0.10 \\)\n",
      "\n",
      "Nos piden:  \n",
      "**Probabilidad de que una persona realmente tenga la enfermedad dado que dio positivo.**  \n",
      "Es decir:  \n",
      "\\( P(E | T^+) = ? \\)\n",
      "\n",
      "---\n",
      "\n",
      "### Usamos el teorema de Bayes:\n",
      "\n",
      "\\[\n",
      "P(E | T^+) = \\frac{P(T^+ | E) \\cdot P(E)}{P(T^+)}\n",
      "\\]\n",
      "\n",
      "Donde \\( P(T^+) \\) es la probabilidad total de dar positivo. Para calcularlo, usamos el **teorema de la probabilidad total**:\n",
      "\n",
      "\\[\n",
      "P(T^+) = P(T^+ | E) \\cdot P(E) + P(T^+ | \\neg E) \\cdot P(\\neg E)\n",
      "\\]\n",
      "\n",
      "Sustituimos los valores:\n",
      "\n",
      "\\[\n",
      "P(T^+) = (0.90)(0.01) + (0.10)(0.99) = 0.009 + 0.099 = 0.108\n",
      "\\]\n",
      "\n",
      "Ahora aplicamos Bayes:\n",
      "\n",
      "\\[\n",
      "P(E | T^+) = \\frac{0.90 \\times 0.01}{0.108} = \\frac{0.009}{0.108} \\approx 0.0833\n",
      "\\]\n",
      "\n",
      "Convertimos a porcentaje:\n",
      "\n",
      "\\[\n",
      "0.0833 \\times 100 \\approx 8.33\\%\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "Respuesta final: 8.33%\n"
     ]
    }
   ],
   "source": [
    "# Prompts SIN CoT y CON CoT (Bayes) ‚Äî nscale-compatible\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "problema = (\n",
    "    \"En una poblaci√≥n, 1% tiene la enfermedad. La prueba tiene 90% de sensibilidad y 90% de especificidad. \"\n",
    "    \"Si una persona da positivo, ¬øcu√°l es la probabilidad (en %) de que realmente est√© enferma?\"\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# SIN CoT: SOLO porcentaje en una l√≠nea (recorta tokens)\n",
    "endpoint_sin = HuggingFaceEndpoint(\n",
    "    repo_id=MODEL_ID, task=\"conversational\",\n",
    "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=6\n",
    ")\n",
    "llm_sin = ChatHuggingFace(llm=endpoint_sin)\n",
    "\n",
    "prompt_sin_cot = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Devuelve SOLO un n√∫mero en formato porcentaje (ej: 8.33%). \"\n",
    "               \"Sin explicaciones, sin ecuaciones, sin texto extra.\"),\n",
    "    (\"human\", \"{q}\")\n",
    "])\n",
    "\n",
    "# CON CoT: piensa paso a paso y cierra con una l√≠nea final\n",
    "endpoint_con = HuggingFaceEndpoint(\n",
    "    repo_id=MODEL_ID, task=\"conversational\",\n",
    "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=256\n",
    ")\n",
    "llm_con = ChatHuggingFace(llm=endpoint_con)\n",
    "\n",
    "prompt_con_cot = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"T√≥mate tu tiempo y piensa paso a paso usando Bayes. \"\n",
    "               \"Al final, da una sola l√≠nea con: 'Respuesta final: <n>%'\"),\n",
    "    (\"human\", \"{q}\")\n",
    "])\n",
    "\n",
    "print(\"=== SIN CoT ===\")\n",
    "print((prompt_sin_cot | llm_sin | parser).invoke({\"q\": problema}))\n",
    "\n",
    "print(\"\\n=== CON CoT ===\")\n",
    "print((prompt_con_cot | llm_con | parser).invoke({\"q\": problema}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXteBwdorB0F"
   },
   "source": [
    "### Self-Consistency / Ensemble prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "7jpZCrBqrB0F",
    "outputId": "94ba1088-4758-41dd-90b8-4f448bd831c5"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1626907301.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     \u001b[0mtmp_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatHuggingFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_endpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtexto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mtmp_llm\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"instruccion\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpregunta\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mrespuestas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3047\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3048\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3049\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3050\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3051\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m         return cast(\n\u001b[1;32m    382\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1005\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m                 results.append(\n\u001b[0;32m--> 825\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    826\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1073\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_huggingface/chat_models/huggingface.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             }\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mllm_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_chat_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         )\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36m_inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_as_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_as_binary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m                 response = get_session().post(\n\u001b[0m\u001b[1;32m    265\u001b[0m                     \u001b[0mrequest_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                     \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Send: {_curlify(request)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_AMZN_TRACE_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ejemplo: self-consistency simple (compatible con nscale/conversational)\n",
    "from collections import Counter\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "pregunta = \"Lista 3 riesgos comunes al usar LLMs en producci√≥n.\"\n",
    "respuestas = []\n",
    "parser = StrOutputParser()\n",
    "\n",
    "for _ in range(5):\n",
    "    tmp_endpoint = HuggingFaceEndpoint(\n",
    "        repo_id=MODEL_ID,\n",
    "        task=\"conversational\",\n",
    "        huggingfacehub_api_token=HF_TOKEN,\n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=200,\n",
    "    )\n",
    "    tmp_llm = ChatHuggingFace(llm=tmp_endpoint)\n",
    "    texto = (prompt | tmp_llm | parser).invoke({\"instruccion\": pregunta})\n",
    "    respuestas.append(texto.strip())\n",
    "\n",
    "conteo = Counter(respuestas)\n",
    "mejor_respuesta, _ = conteo.most_common(1)[0]\n",
    "\n",
    "print(\"\\n==== Candidatas ====\")\n",
    "for i, r in enumerate(respuestas, 1):\n",
    "    print(f\"[{i}]\\n{r}\\n\")\n",
    "\n",
    "print(\"==== Seleccionada por frecuencia ====\")\n",
    "print(mejor_respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SPYi9X1rB0F"
   },
   "source": [
    "### Salida estructurada (JSON Output Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCegKKRDrB0F",
    "outputId": "0a6642d1-926b-4683-8e6a-0da447fc7e05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"titulo\": \"RAG (Retrieval-Augmented Generation)\",\n",
      "  \"puntos_clave\": [\n",
      "    \"RAG combina la recuperaci√≥n de informaci√≥n de una base de conocimientos con la generaci√≥n de texto para mejorar la precisi√≥n y relevancia.\",\n",
      "    \"Permite que los modelos de lenguaje utilicen informaci√≥n de documentos externos en tiempo real durante la generaci√≥n de respuestas.\",\n",
      "    \"Reduce el riesgo de generar contenido fabricado al basarse en fuentes verificadas de informaci√≥n.\"\n",
      "  ],\n",
      "  \"dificultad\": \"intermedio\"\n",
      "}\n",
      "JSON v√°lido con las claves requeridas.\n"
     ]
    }
   ],
   "source": [
    "# Salida estructurada (JSON) con output parser\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "json_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Eres un asistente que devuelve SIEMPRE JSON v√°lido. Dado un tema, devuelve un objeto con:\n",
    "    - \"titulo\": string\n",
    "    - \"puntos_clave\": lista de 3 strings\n",
    "    - \"dificultad\": uno de [\"b√°sico\", \"intermedio\", \"avanzado\"]\n",
    "    Responde SOLO con JSON v√°lido sin texto adicional.\n",
    "    Tema: {tema}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "json_text = (json_prompt | llm | StrOutputParser()).invoke({\"tema\": \"RAG\"})\n",
    "print(json_text)\n",
    "\n",
    "data = json.loads(json_text)\n",
    "assert set([\"titulo\", \"puntos_clave\", \"dificultad\"]).issubset(data.keys())\n",
    "print(\"JSON v√°lido con las claves requeridas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC4DngMVrB0G"
   },
   "source": [
    "## Ejercicios ‚Äî Parte 2\n",
    "\n",
    "Resuelve los siguientes ejercicios. Modifica prompts y par√°metros si es necesario y justifica brevemente tus decisiones (en una celda de texto).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFAmf9D0rB0G"
   },
   "source": [
    "### Ejercicio 2.1 ‚Äî Zero-shot vs Few-shot\n",
    "\n",
    "- Tarea: explicar ‚Äúregularizaci√≥n L2‚Äù en 3 vi√±etas claras para un p√∫blico t√©cnico.\n",
    "- Paso 1 (zero-shot): crea un prompt sin ejemplos y observa el resultado.\n",
    "- Paso 2 (few-shot): agrega 1-2 ejemplos de estilo y compara la salida.\n",
    "- Pregunta gu√≠a: ¬ømejor√≥ la precisi√≥n o claridad con pocos ejemplos? Justifica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fd2vCaKlrB0G",
    "outputId": "9b6ac954-82d1-49b7-adb9-1794ee3e248c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ZERO-SHOT ===\n",
      "\n",
      "- La **regularizaci√≥n L2** a√±ade un t√©rmino al costo de p√©rdida (loss function) que penaliza el tama√±o de los coeficientes del modelo, espec√≠ficamente sumando el cuadrado de todos los pesos, multiplicado por un par√°metro de regularizaci√≥n Œª.  \n",
      "- Este enfoque tiende a **reducir el overfitting** al promover soluciones con coeficientes m√°s peque√±os y distribuidos, lo que implica una modelaci√≥n m√°s generalizable.  \n",
      "- En t√©rminos matem√°ticos, si la funci√≥n de p√©rdida original es $ \\mathcal{L} $, la versi√≥n regularizada se expresa como $ \\mathcal{L} + \\lambda \\sum_{i} w_i^2 $, donde $ w_i $ son los coeficientes del modelo y $ \\lambda $ controla el grado de penalizaci√≥n.\n",
      "\n",
      "=== FEW-SHOT ===\n",
      "\n",
      "- Penaliza cuadrado de magnitud de coeficientes  \n",
      "- Promueve coeficientes peque√±os pero no nulos  \n",
      "- Ayuda a evitar sobreajuste en modelos de regresi√≥n\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 2.1 ‚Äî Zero-shot vs Few-shot (c√≥digo)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Zero-shot\n",
    "zero_shot_A = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente t√©cnico y claro. Responde en vi√±etas.\"),\n",
    "    (\"human\", \"Explica 'regularizaci√≥n L2' en 3 vi√±etas para un p√∫blico t√©cnico.\"),\n",
    "])\n",
    "print(\"\\n=== ZERO-SHOT ===\\n\")\n",
    "print((zero_shot_A | llm | StrOutputParser()).invoke({}))\n",
    "\n",
    "# Few-shot (agregar 1-2 ejemplos de estilo)\n",
    "few_shot_A = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente t√©cnico y claro. Responde en vi√±etas.\"),\n",
    "    (\"human\", \"Explica 'regularizaci√≥n L1' en 3 vi√±etas\"),\n",
    "    (\"ai\", \"- Penaliza magnitud absoluta\\n- Induce sparsidad en coeficientes\\n- √ötil para selecci√≥n de variables\"),\n",
    "    (\"human\", \"Explica 'regularizaci√≥n L2' en 3 vi√±etas\"),\n",
    "])\n",
    "print(\"\\n=== FEW-SHOT ===\\n\")\n",
    "print((few_shot_A | llm | StrOutputParser()).invoke({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsrH2ZdtrB0G"
   },
   "source": [
    "### Ejercicio 2.2 ‚Äî Chain-of-Thought (CoT)\n",
    "\n",
    "- Tarea: dado un problema de evaluaci√≥n de modelos, razonar paso a paso y entregar una conclusi√≥n final breve.\n",
    "- Problema sugerido: ‚Äú¬øPor qu√© accuracy puede ser enga√±oso en un dataset muy desbalanceado y qu√© m√©trica alternativa usar√≠as?‚Äù\n",
    "- Pista: pide expl√≠citamente ‚Äúrazona paso a paso y luego da una respuesta final breve en una l√≠nea‚Äù.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37-CDC6irB0G",
    "outputId": "a73ad459-a554-478d-a8b4-2aa2b46d9959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 1: En un dataset desbalanceado, la mayor√≠a de las instancias pertenecen a una clase, por lo que un modelo puede predecir solo esa clase con alta frecuencia y obtener una accuracy alta, aunque sea in√∫til pr√°cticamente.\n",
      "\n",
      "Paso 2: Esta alta accuracy es enga√±osa porque no refleja el desempe√±o real en las clases minoritarias, que suelen ser m√°s importantes en aplicaciones reales.\n",
      "\n",
      "Paso 3: Una m√©trica alternativa que considera mejor el desempe√±o en todas las clases es la precisi√≥n (precision), el recall (o tasa de detecci√≥n), o el F1-score, especialmente el F1-score, que es el promedio armonicamente entre precisi√≥n y recall.\n",
      "\n",
      "Paso 4: El F1-score ofrece un balance adecuado entre precisi√≥n y recall, y es m√°s robusto en datos desbalanceados.\n",
      "\n",
      "Respuesta final: La accuracy puede ser enga√±oso en datasets desbalanceados porque favorece la clase mayoritaria; se recomienda usar el F1-score como m√©trica alternativa.\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 2.2 ‚Äî CoT (c√≥digo)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "cot_ex = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un tutor de IA. Razona paso a paso y luego da una respuesta final breve.\"),\n",
    "    (\"human\", \"Problema: {problema}\\nIndica tus pasos de razonamiento y luego una respuesta final en una sola l√≠nea.\"),\n",
    "])\n",
    "\n",
    "problema = \"¬øPor qu√© accuracy puede ser enga√±oso en un dataset muy desbalanceado y qu√© m√©trica alternativa usar√≠as?\"\n",
    "print((cot_ex | llm | StrOutputParser()).invoke({\"problema\": problema}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRDMh9ewrB0G"
   },
   "source": [
    "### Ejercicio 2.3 ‚Äî Role prompting\n",
    "\n",
    "- Tarea: explicar el ‚Äúsesgo de selecci√≥n‚Äù a un equipo de data engineering con ejemplos concisos.\n",
    "- Rol: ‚ÄúAct√∫a como l√≠der t√©cnico de datos; s√© pragm√°tico y directo, con vi√±etas concretas‚Äù.\n",
    "- Objetivo: evaluar c√≥mo cambia el estilo bajo un rol t√©cnico espec√≠fico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnGf_iFWrB0G",
    "outputId": "d8d72898-8cd6-4c23-9519-36f245d590bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **El sesgo de selecci√≥n ocurre cuando los datos usados para entrenar un modelo o tomar decisiones no representan adecuadamente a toda la poblaci√≥n objetivo**, lo que lleva a predicciones o conclusiones sesgadas.  \n",
      "- **En data engineering, esto suele manifestarse en la forma en que se recolectan, filtran o procesan los datos**: por ejemplo, solo usar registros de usuarios activos en una aplicaci√≥n, ignorando a los inactivos.  \n",
      "- **El riesgo es que los modelos se entrenen en subconjuntos distorsionados**, resultando en decisiones que no se generalizan bien (como un algoritmo de recomendaci√≥n que nunca sugiere contenidos para usuarios pasivos).\n",
      "\n",
      "**Ejemplo pr√°ctico**:  \n",
      "Un equipo de datos construye un sistema de detecci√≥n de fraudes que solo se entrena con transacciones de usuarios de cierto pa√≠s (por ejemplo, EE.UU.). Como el perfil de transacciones y comportamientos de fraudes var√≠an en otros pa√≠ses, el modelo falla al detectar fraudes en transacciones de usuarios de Espa√±a, por lo que **el sesgo de selecci√≥n distorsiona la capacidad del sistema para generalizarse**.\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 2.3 ‚Äî Role prompting (c√≥digo)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "role_ex = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Act√∫a como l√≠der t√©cnico de datos; s√© pragm√°tico y directo, con vi√±etas concretas.\"),\n",
    "    (\"human\", \"Explica el ‚Äòsesgo de selecci√≥n‚Äô a un equipo de data engineering en 3 vi√±etas y da 1 ejemplo pr√°ctico.\"),\n",
    "])\n",
    "\n",
    "print((role_ex | llm | StrOutputParser()).invoke({}))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
