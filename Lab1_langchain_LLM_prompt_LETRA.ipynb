{"cells":[{"cell_type":"markdown","metadata":{"id":"VBCSNaCdrBz-"},"source":["# Laboratorio: Setup y uso básico de LLMs con LangChain + Prompt engineering avanzado\n","\n","Este laboratorio está pensado para completarse en ~2 horas. Trabajaremos en Google Colab con recursos gratuitos, usando la Inference API de Hugging Face y un modelo instruct abierto.\n","\n","- Contenidos:\n","  - Setup en Colab y configuración de Hugging Face Inference API\n","  - Uso básico de LLMs con LangChain (LCEL)\n","  - Parámetros de decodificación y control de estilo\n","  - Prompt engineering avanzado: zero-shot, few-shot, Chain of Thought, Role Prompting y salida estructurada (JSON)\n","\n","Al finalizar, podrás:\n","- Conectarte a un LLM instruct vía Hugging Face Inference API desde LangChain.\n","- Construir cadenas simples con `prompt | llm | parser`.\n","- Diseñar prompts efectivos y controlar formato de salida (incluido JSON).\n"]},{"cell_type":"markdown","metadata":{"id":"4RTG7_ybrBz_"},"source":["## Parte 0 — Setup (Colab + librerías + token)\n","\n","Usaremos versiones estables para minimizar fricción en Colab. Asegurate de ejecutar esta sección antes de continuar.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cxoJSkLcrBz_","outputId":"36ff7967-4de2-4f8e-ab65-bbd925fa6295"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.0 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/443.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.2/373.2 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Instalar dependencias principales (versiones estables)\n","!pip -q install -U \\\n","  \"langchain==0.3.27\" \\\n","  \"langchain-community==0.3.27\" \\\n","  \"langchain-huggingface==0.3.1\" \\\n","  \"transformers==4.55.2\" \\\n","  \"huggingface_hub==0.34.4\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OmVbT5uRxn5R","outputId":"4bf9c413-6e72-433b-9d10-fd1cb0910438"},"outputs":[{"name":"stdout","output_type":"stream","text":["langchain => 0.3.27\n","langchain-community => 0.3.27\n","langchain-huggingface => 0.3.1\n","transformers => 4.55.2\n","huggingface_hub => 0.34.4\n"]}],"source":["from importlib.metadata import version, PackageNotFoundError\n","\n","for dist in [\"langchain\", \"langchain-community\", \"langchain-huggingface\",\n","             \"transformers\", \"huggingface_hub\"]:\n","    try:\n","        print(dist, \"=>\", version(dist))\n","    except PackageNotFoundError:\n","        print(dist, \"no instalado\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M9EJxPOCrB0A","outputId":"3f389cf0-dc68-4fc4-8fb2-9c56ef6a4a5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n","PyTorch: 2.6.0+cu124\n","CUDA disponible: True\n"]}],"source":["# Comprobar versión de Python y GPU/CPU\n","import sys, subprocess, torch\n","print(sys.version)\n","try:\n","    import torch\n","    print(\"PyTorch:\", torch.__version__)\n","    print(\"CUDA disponible:\", torch.cuda.is_available())\n","except Exception as e:\n","    print(\"PyTorch no disponible o sin CUDA\", e)\n"]},{"cell_type":"markdown","metadata":{"id":"2Ug4tn-VrB0A"},"source":["### Configuración del token de Hugging Face\n","\n","Para usar la Hugging Face Inference API, necesitás un token personal (gratuito). En Colab se recomienda guardarlo en `userdata`:\n","\n","1. Crear el token en `https://huggingface.co/settings/tokens`.\n","2. En Colab: Abre el menú en la barra izquierda haciendo click en la llave → \"Agregar nuevo secreto\" ponle `HF_TOKEN` y el token que generamos → Habilita el acceso al notebook.\n","3. Ejecutar la celda siguiente para leerlo.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DMTYDzh9rB0A","outputId":"e2489466-05ee-400f-c153-9158abd3559e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Token cargado OK\n"]}],"source":["from google.colab import userdata\n","HF_TOKEN = userdata.get('HF_TOKEN')\n","assert HF_TOKEN is not None and len(HF_TOKEN) > 0, \"Configurar el secreto 'HF_TOKEN' en Colab.\"\n","print(\"Token cargado OK\")\n"]},{"cell_type":"markdown","metadata":{"id":"VQbt6t3FrB0A"},"source":["## Parte 1 — Uso básico de LLMs con LangChain\n","\n","Trabajaremos con un modelo instruct accesible vía Inference API. Para minimizar fricción, usaremos un modelo abierto.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fNzk__gZrB0B","outputId":"d7e9cd03-2f9a-4ced-dc22-c7e5f617f510"},"outputs":[{"name":"stdout","output_type":"stream","text":["Un LLM (Large Language Model) es un tipo de modelo de inteligencia artificial entrenado en grandes volúmenes de texto para entender, generar y realizar tareas relacionadas con el lenguaje humano.  \n","Casos de uso incluyen responder preguntas de forma natural y generar textos como artículos, emails o historias.  \n","También se usan en asistentes virtuales, traducción automática y análisis de sentimientos.\n"]}],"source":["from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n","\n","hf_endpoint = HuggingFaceEndpoint(\n","    repo_id=MODEL_ID,\n","    task=\"conversational\",\n","    huggingfacehub_api_token=HF_TOKEN,\n","    temperature=0.7,\n","    max_new_tokens=256,\n",")\n","\n","llm = ChatHuggingFace(llm=hf_endpoint)\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Eres un asistente útil y conciso.\"),\n","    (\"human\", \"Responde a la siguiente instrucción: {instruccion}\"),\n","])\n","\n","chain = prompt | llm | StrOutputParser()\n","print(chain.invoke({\"instruccion\": \"Explica en 3 frases qué es un LLM y nombra 2 casos de uso.\"}))"]},{"cell_type":"markdown","metadata":{"id":"JM0fs9ZLrB0B"},"source":["### Ejercicio 1.1 (10 min)\n","\n","- Probar 3 variaciones de `temperature` y observar el cambio en estilo.\n","- Cambiar el rol del `system` para forzar un estilo (p.ej., “responde con viñetas y máximo 3 líneas”).\n","- Pregunta sugerida: “Resume la diferencia entre entrenamiento y fine-tuning en 3 puntos.”\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mcTYMM-srB0B"},"outputs":[],"source":["from typing import List\n","\n","instruccion: str = \"Resume la diferencia entre entrenamiento y fine-tuning en 3 puntos.\"\n","temperaturas: List[float] = [0.0, 0.7, 1.2]\n","\n","def ejecutar_variacion(temperature: float) -> str:\n","    # TODO: crear endpoint conversacional con 'temperature' y devolver el texto\n","    # Debe usar: MODEL_ID, HF_TOKEN, task=\"conversational\"\n","    raise NotImplementedError\n","\n","for t in temperaturas:\n","    print(f\"\\n==== temperature: {t} ====\")\n","    print(ejecutar_variacion(t))\n","\n","def ejecutar_estilo(instruccion: str) -> str:\n","    # TODO: crear prompt de estilo (system) + LLM base conversacional y devolver el texto\n","    raise NotImplementedError\n","\n","print(\"\\n==== estilo forzado ====\")\n","print(ejecutar_estilo(instruccion))"]},{"cell_type":"markdown","metadata":{"id":"Mi_Cg8IMrB0C"},"source":["## Parte 1.2 — Parámetros de decodificación\n","\n","Ajustaremos parámetros como `top_p`, `repetition_penalty` y `max_new_tokens` para observar su efecto en la generación.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lCnXq_QrB0C"},"outputs":[],"source":["# Exploración de parámetros de decodificación\n","from typing import Dict, Any\n","from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n","from langchain_core.output_parsers import StrOutputParser\n","\n","consulta: str = \"Escribe una analogía breve para explicar RAG a un público no técnico.\"\n","\n","configuraciones: Dict[str, Dict[str, Any]] = {\n","    \"baseline\":   {\"temperature\": 0.7, \"top_p\": 0.95, \"repetition_penalty\": 1.0, \"max_new_tokens\": 128},\n","    \"creativo\":   {\"temperature\": 1.1, \"top_p\": 0.90, \"repetition_penalty\": 1.0, \"max_new_tokens\": 128},\n","    \"controlado\": {\"temperature\": 0.2, \"top_p\": 0.80, \"repetition_penalty\": 1.1, \"max_new_tokens\": 96},\n","}\n","\n","for nombre, cfg in configuraciones.items():\n","    tmp_endpoint = HuggingFaceEndpoint(\n","        repo_id=MODEL_ID,\n","        task=\"conversational\",\n","        huggingfacehub_api_token=HF_TOKEN,\n","        temperature=cfg[\"temperature\"],\n","        top_p=cfg[\"top_p\"],\n","        repetition_penalty=cfg[\"repetition_penalty\"],\n","        max_new_tokens=cfg[\"max_new_tokens\"],\n","    )\n","    tmp_llm = ChatHuggingFace(llm=tmp_endpoint)\n","    salida = (prompt | tmp_llm | StrOutputParser()).invoke({\"instruccion\": consulta})\n","    print(f\"\\n==== {nombre} ({cfg}) ====\\n{salida}\")"]},{"cell_type":"markdown","metadata":{"id":"PFWQ7vryrB0C"},"source":["## Parte 2 — Prompt engineering avanzado\n","\n","Exploraremos estrategias para mejorar la calidad y control de las respuestas: zero-shot, few-shot, restricciones de estilo, salida estructurada y Chain of Thought.\n"]},{"cell_type":"markdown","metadata":{"id":"n1U5MiS1rB0C"},"source":["### Conceptos clave: Zero-shot, Few-shot, CoT y Roles\n","\n","- Zero-shot: el modelo resuelve la tarea solo con instrucciones; no se proporcionan ejemplos.\n","- Few-shot: además de la instrucción, se incluyen 1-3 ejemplos que muestran el formato y estilo deseados.\n","- Chain-of-Thought (CoT): se guía al modelo para mostrar pasos intermedios de razonamiento (p.ej., “razona paso a paso”) antes de una respuesta final.\n","- Role prompting: se asigna un rol (p.ej., “actúa como profesor de IA”) para influir en el estilo y nivel de detalle.\n","- JSON output: se pide una salida estricta en formato JSON y se valida con un parser.\n","\n","Lectura recomendada: guía de prompt engineering avanzada en `https://learnprompting.org/docs/introduction`.\n"]},{"cell_type":"markdown","metadata":{"id":"LiF9jJRfrB0C"},"source":["### Zero-shot y Few-shot\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fNZ9KnrLrB0D","outputId":"0d87f5fd-e84b-4da8-d926-d12e9494575f"},"outputs":[{"name":"stdout","output_type":"stream","text":["=== ZERO-SHOT ===\n","Overfitting occurs when a model learns noise instead of patterns.  \n","Overcomplicated models fail on new, unseen data.  \n","Overfitting reduces generalization in predictions.\n","\n","=== FEW-SHOT ===\n","O Viola la generalización al ajustarse demasiado a datos de entrenamiento  \n","V Evalúa mal en datos nuevos por falta de robustez  \n","E Excede el poder de representación del modelo\n"]}],"source":["# Zero-shot vs Few-shot (diferencia marcada con patrón acróstico)\n","from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# LLM más determinista\n","det_endpoint = HuggingFaceEndpoint(\n","    repo_id=MODEL_ID, task=\"conversational\",\n","    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=128\n",")\n","llm_det = ChatHuggingFace(llm=det_endpoint)\n","parser = StrOutputParser()\n","\n","# Patrón no obvio: acróstico O-V-E (Overfitting), cada línea empieza con esa letra\n","instruccion = (\n","    \"Escribe EXACTAMENTE 3 líneas sobre 'overfitting'. \"\n","    \"Cada línea debe comenzar con O, luego V, luego E (en ese orden). \"\n","    \"6-10 palabras por línea. Sin texto extra.\"\n",")\n","\n","zero_shot = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Sigue estrictamente las instrucciones del usuario.\"),\n","    (\"human\", \"{instruccion}\")\n","])\n","\n","few_shot = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Sigue estrictamente las instrucciones del usuario.\"),\n","    # Ejemplo: el patrón se demuestra con otro tema y otro acróstico (R-E-G)\n","    (\"human\", \"Escribe EXACTAMENTE 3 líneas sobre 'regularización'. \"\n","              \"Cada línea debe comenzar con R, luego E, luego G. \"\n","              \"6-10 palabras por línea. Sin texto extra.\"),\n","    (\"ai\", \"R Reduce complejidad para evitar ajustes al ruido\\n\"\n","           \"E Estabiliza el aprendizaje con penalizaciones adecuadas\\n\"\n","           \"G Generaliza mejor limitando pesos excesivamente grandes\"),\n","    # Ahora se pide el caso real con el acróstico O-V-E\n","    (\"human\", \"{instruccion}\")\n","])\n","\n","print(\"=== ZERO-SHOT ===\")\n","print((zero_shot | llm_det | parser).invoke({\"instruccion\": instruccion}))\n","\n","print(\"\\n=== FEW-SHOT ===\")\n","print((few_shot | llm_det | parser).invoke({\"instruccion\": instruccion}))"]},{"cell_type":"markdown","metadata":{"id":"4b7hTF4wrB0D"},"source":["### Role prompting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_VhEgQZrB0D","outputId":"fd5e846d-cafc-4dff-fb4f-f762db9e9cd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Claro, aquí tienes una explicación breve y clara:\n","\n","**Qué es el aprendizaje por refuerzo (Reinforcement Learning):**  \n","Es un tipo de inteligencia artificial donde un agente (como un robot o un programa) aprende a tomar decisiones para maximizar una recompensa a lo largo del tiempo. No tiene acceso a datos etiquetados, sino que aprende probando diferentes acciones y viéndose recompensado o sancionado según los resultados.\n","\n","🔍 **Por ejemplo:**  \n","Imagina que un agente quiere aprender a jugar al ajedrez. Cada vez que gana, recibe una recompensa. Si pierde, la recompensa es baja o negativa. Con el tiempo, el agente aprende qué movimientos le llevan a ganar más.\n","\n","### 2 ejemplos de aplicación:\n","1. **Juegos de video** (como AlphaGo o Deep Q-Networks): Los algoritmos aprenden a jugar mejor jugando miles de partidas y recibiendo recompensas por ganar.\n","2. **Robótica y automatización** (como un robot que aprende a caminar o navegar): El robot recibe recompensas cuando se mueve bien o evita caídas, y ajusta su comportamiento para mejorar.\n","\n","En resumen: **El aprendizaje por refuerzo es aprender por intentos, errores y recompensas.**\n"]}],"source":["# Role prompting\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","role_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Actúa como profesor de IA de nivel intermedio. Sé claro, estructurado y usa ejemplos sencillos.\"),\n","    (\"human\", \"Explica brevemente qué es el aprendizaje por refuerzo y menciona 2 ejemplos de aplicación.\"),\n","])\n","\n","print((role_prompt | llm | StrOutputParser()).invoke({}))\n"]},{"cell_type":"markdown","metadata":{"id":"BFhoZDI4rB0D"},"source":["### CoT (Chain-of-Thought) prompting"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9AK2aANwrB0D","outputId":"2e40d0fd-99ee-4a28-943a-63dc34811eeb"},"outputs":[{"name":"stdout","output_type":"stream","text":["=== SIN CoT ===\n","9.09%\n","\n","=== CON CoT ===\n","Vamos a resolver este problema paso a paso usando el **teorema de Bayes**.\n","\n","---\n","\n","### Datos del problema:\n","\n","- Probabilidad de tener la enfermedad (prevalencia):  \n","  \\( P(E) = 1\\% = 0.01 \\)\n","\n","- Probabilidad de no tener la enfermedad:  \n","  \\( P(\\neg E) = 1 - 0.01 = 0.99 \\)\n","\n","- Sensibilidad de la prueba (probabilidad de dar positivo si tienes la enfermedad):  \n","  \\( P(T^+ | E) = 90\\% = 0.90 \\)\n","\n","- Especificidad de la prueba (probabilidad de dar negativo si no tienes la enfermedad):  \n","  \\( P(T^- | \\neg E) = 90\\% = 0.90 \\)\n","\n","Esto implica que:\n","\n","- El error de falsa positiva es \\( P(T^+ | \\neg E) = 1 - 0.90 = 0.10 \\)\n","\n","Nos piden:  \n","**Probabilidad de que una persona realmente tenga la enfermedad dado que dio positivo.**  \n","Es decir:  \n","\\( P(E | T^+) = ? \\)\n","\n","---\n","\n","### Usamos el teorema de Bayes:\n","\n","\\[\n","P(E | T^+) = \\frac{P(T^+ | E) \\cdot P(E)}{P(T^+)}\n","\\]\n","\n","Donde \\( P(T^+) \\) es la probabilidad total de dar positivo. Para calcularlo, usamos el **teorema de la probabilidad total**:\n","\n","\\[\n","P(T^+) = P(T^+ | E) \\cdot P(E) + P(T^+ | \\neg E) \\cdot P(\\neg E)\n","\\]\n","\n","Sustituimos los valores:\n","\n","\\[\n","P(T^+) = (0.90)(0.01) + (0.10)(0.99) = 0.009 + 0.099 = 0.108\n","\\]\n","\n","Ahora aplicamos Bayes:\n","\n","\\[\n","P(E | T^+) = \\frac{0.90 \\times 0.01}{0.108} = \\frac{0.009}{0.108} \\approx 0.0833\n","\\]\n","\n","Convertimos a porcentaje:\n","\n","\\[\n","0.0833 \\times 100 \\approx 8.33\\%\n","\\]\n","\n","---\n","\n","Respuesta final: 8.33%\n"]}],"source":["# Prompts SIN CoT y CON CoT (Bayes) — nscale-compatible\n","\n","from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","problema = (\n","    \"En una población, 1% tiene la enfermedad. La prueba tiene 90% de sensibilidad y 90% de especificidad. \"\n","    \"Si una persona da positivo, ¿cuál es la probabilidad (en %) de que realmente esté enferma?\"\n",")\n","\n","parser = StrOutputParser()\n","\n","# SIN CoT: SOLO porcentaje en una línea (recorta tokens)\n","endpoint_sin = HuggingFaceEndpoint(\n","    repo_id=MODEL_ID, task=\"conversational\",\n","    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=6\n",")\n","llm_sin = ChatHuggingFace(llm=endpoint_sin)\n","\n","prompt_sin_cot = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Devuelve SOLO un número en formato porcentaje (ej: 8.33%). \"\n","               \"Sin explicaciones, sin ecuaciones, sin texto extra.\"),\n","    (\"human\", \"{q}\")\n","])\n","\n","# CON CoT: piensa paso a paso y cierra con una línea final\n","endpoint_con = HuggingFaceEndpoint(\n","    repo_id=MODEL_ID, task=\"conversational\",\n","    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=256\n",")\n","llm_con = ChatHuggingFace(llm=endpoint_con)\n","\n","prompt_con_cot = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Tómate tu tiempo y piensa paso a paso usando Bayes. \"\n","               \"Al final, da una sola línea con: 'Respuesta final: <n>%'\"),\n","    (\"human\", \"{q}\")\n","])\n","\n","print(\"=== SIN CoT ===\")\n","print((prompt_sin_cot | llm_sin | parser).invoke({\"q\": problema}))\n","\n","print(\"\\n=== CON CoT ===\")\n","print((prompt_con_cot | llm_con | parser).invoke({\"q\": problema}))"]},{"cell_type":"markdown","metadata":{"id":"6SPYi9X1rB0F"},"source":["### Salida estructurada (JSON Output Parser)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mCegKKRDrB0F","outputId":"0a6642d1-926b-4683-8e6a-0da447fc7e05"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"titulo\": \"RAG (Retrieval-Augmented Generation)\",\n","  \"puntos_clave\": [\n","    \"RAG combina la recuperación de información de una base de conocimientos con la generación de texto para mejorar la precisión y relevancia.\",\n","    \"Permite que los modelos de lenguaje utilicen información de documentos externos en tiempo real durante la generación de respuestas.\",\n","    \"Reduce el riesgo de generar contenido fabricado al basarse en fuentes verificadas de información.\"\n","  ],\n","  \"dificultad\": \"intermedio\"\n","}\n","JSON válido con las claves requeridas.\n"]}],"source":["# Salida estructurada (JSON) con output parser\n","import json\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","json_prompt = ChatPromptTemplate.from_template(\n","    \"\"\"\n","    Eres un asistente que devuelve SIEMPRE JSON válido. Dado un tema, devuelve un objeto con:\n","    - \"titulo\": string\n","    - \"puntos_clave\": lista de 3 strings\n","    - \"dificultad\": uno de [\"básico\", \"intermedio\", \"avanzado\"]\n","    Responde SOLO con JSON válido sin texto adicional.\n","    Tema: {tema}\n","    \"\"\"\n",")\n","\n","json_text = (json_prompt | llm | StrOutputParser()).invoke({\"tema\": \"RAG\"})\n","print(json_text)\n","\n","data = json.loads(json_text)\n","assert set([\"titulo\", \"puntos_clave\", \"dificultad\"]).issubset(data.keys())\n","print(\"JSON válido con las claves requeridas.\")\n"]},{"cell_type":"markdown","metadata":{"id":"CC4DngMVrB0G"},"source":["## Ejercicios — Parte 2\n","\n","Resuelve los siguientes ejercicios. Modifica prompts y parámetros si es necesario y justifica brevemente tus decisiones (en una celda de texto).\n"]},{"cell_type":"markdown","metadata":{"id":"OFAmf9D0rB0G"},"source":["### Ejercicio 2.1 — Zero-shot vs Few-shot\n","\n","- Tarea: explicar “regularización L2” en 3 viñetas claras para un público técnico.\n","- Paso 1 (zero-shot): crea un prompt sin ejemplos y observa el resultado.\n","- Paso 2 (few-shot): agrega 1-2 ejemplos de estilo y compara la salida.\n","- Pregunta guía: ¿mejoró la precisión o claridad con pocos ejemplos? Justifica.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fd2vCaKlrB0G"},"outputs":[],"source":["def zero_shot_l2() -> str:\n","    # TODO: construir prompt zero-shot (3 viñetas) y llamar al LLM\n","    raise NotImplementedError\n","\n","def few_shot_l2() -> str:\n","    # TODO: construir prompt con 1-2 ejemplos y llamar al LLM\n","    raise NotImplementedError\n","\n","print(\"\\n=== ZERO-SHOT ===\\n\")\n","print(zero_shot_l2())\n","\n","print(\"\\n=== FEW-SHOT ===\\n\")\n","print(few_shot_l2())"]},{"cell_type":"markdown","metadata":{"id":"gsrH2ZdtrB0G"},"source":["### Ejercicio 2.2 — Chain-of-Thought (CoT)\n","\n","- Tarea: dado un problema de evaluación de modelos, razonar paso a paso y entregar una conclusión final breve.\n","- Problema sugerido: “¿Por qué accuracy puede ser engañoso en un dataset muy desbalanceado y qué métrica alternativa usarías?”\n","- Pista: pide explícitamente “razona paso a paso y luego da una respuesta final breve en una línea”.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"37-CDC6irB0G"},"outputs":[],"source":["def cot_razonamiento(problema: str) -> str:\n","    # TODO: pedir \"razona paso a paso\" y cerrar con una línea final\n","    raise NotImplementedError\n","\n","problema: str = \"¿Por qué accuracy puede ser engañoso en un dataset muy desbalanceado y qué métrica alternativa usarías?\"\n","print(cot_razonamiento(problema))"]},{"cell_type":"markdown","metadata":{"id":"cRDMh9ewrB0G"},"source":["### Ejercicio 2.3 — Role prompting\n","\n","- Tarea: explicar el “sesgo de selección” a un equipo de data engineering con ejemplos concisos.\n","- Rol: “Actúa como líder técnico de datos; sé pragmático y directo, con viñetas concretas”.\n","- Objetivo: evaluar cómo cambia el estilo bajo un rol técnico específico.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnGf_iFWrB0G"},"outputs":[],"source":["def explicar_sesgo_seleccion() -> str:\n","    # TODO: role \"líder técnico de datos\", 3 viñetas + 1 ejemplo práctico\n","    raise NotImplementedError\n","\n","print(explicar_sesgo_seleccion())"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}