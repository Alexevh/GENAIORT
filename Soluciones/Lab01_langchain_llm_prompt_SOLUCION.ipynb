{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBCSNaCdrBz-"
   },
   "source": [
    "# Laboratorio: Setup y uso básico de LLMs con LangChain + Prompt engineering avanzado\n",
    "\n",
    "Este laboratorio está pensado para completarse en ~2 horas. Trabajaremos en Google Colab con recursos gratuitos, usando la Inference API de Hugging Face y un modelo instruct abierto.\n",
    "\n",
    "- Contenidos:\n",
    "  - Setup en Colab y configuración de Hugging Face Inference API\n",
    "  - Uso básico de LLMs con LangChain (LCEL)\n",
    "  - Parámetros de decodificación y control de estilo\n",
    "  - Prompt engineering avanzado: zero-shot, few-shot, Chain of Thought y salida estructurada (JSON)\n",
    "\n",
    "Al finalizar, podrás:\n",
    "- Conectarte a un LLM instruct vía Hugging Face Inference API desde LangChain.\n",
    "- Construir cadenas simples con `prompt | llm | parser`.\n",
    "- Diseñar prompts efectivos y controlar formato de salida (incluido JSON).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RTG7_ybrBz_"
   },
   "source": [
    "## Parte 0 — Setup (Colab + librerías + token)\n",
    "\n",
    "Usaremos versiones estables para minimizar fricción en Colab. Asegurate de ejecutar esta sección antes de continuar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29325,
     "status": "ok",
     "timestamp": 1756393838666,
     "user": {
      "displayName": "Joaquin Bonifacino",
      "userId": "00057501700847538249"
     },
     "user_tz": 180
    },
    "id": "cxoJSkLcrBz_",
    "outputId": "7440883d-b45f-486f-e9f6-9bc9e47d9a42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Instalar dependencias principales (versiones estables)\n",
    "!pip -q install -U \\\n",
    "  \"langchain==0.3.27\" \\\n",
    "  \"langchain-community==0.3.27\" \\\n",
    "  \"langchain-huggingface==0.3.1\" \\\n",
    "  \"transformers==4.55.2\" \\\n",
    "  \"huggingface_hub==0.34.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1756393838910,
     "user": {
      "displayName": "Joaquin Bonifacino",
      "userId": "00057501700847538249"
     },
     "user_tz": 180
    },
    "id": "OmVbT5uRxn5R",
    "outputId": "7476bd33-3811-4185-e06f-e9f7836944da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain => 0.3.27\n",
      "langchain-community => 0.3.27\n",
      "langchain-huggingface => 0.3.1\n",
      "transformers => 4.55.2\n",
      "huggingface_hub => 0.34.4\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version, PackageNotFoundError\n",
    "\n",
    "for dist in [\"langchain\", \"langchain-community\", \"langchain-huggingface\",\n",
    "             \"transformers\", \"huggingface_hub\"]:\n",
    "    try:\n",
    "        print(dist, \"=>\", version(dist))\n",
    "    except PackageNotFoundError:\n",
    "        print(dist, \"no instalado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7955,
     "status": "ok",
     "timestamp": 1756393846866,
     "user": {
      "displayName": "Joaquin Bonifacino",
      "userId": "00057501700847538249"
     },
     "user_tz": 180
    },
    "id": "M9EJxPOCrB0A",
    "outputId": "50248f2c-21c8-431c-fdae-4d7d951efa16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
      "PyTorch: 2.8.0+cu126\n",
      "CUDA disponible: False\n"
     ]
    }
   ],
   "source": [
    "# Comprobar versión de Python y GPU/CPU\n",
    "import sys, subprocess, torch\n",
    "print(sys.version)\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "except Exception as e:\n",
    "    print(\"PyTorch no disponible o sin CUDA\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ug4tn-VrB0A"
   },
   "source": [
    "### Configuración del token de Hugging Face\n",
    "\n",
    "Para usar la Hugging Face Inference API, necesitás un token personal (gratuito). En Colab se recomienda guardarlo en `userdata`:\n",
    "\n",
    "1. Crear el token en `https://huggingface.co/settings/tokens`.\n",
    "2. En Colab: Abre el menu en la barra izquierda haciendo click en la llave → \"Agregar nuevo secreto\" ponle `HF_TOKEN` y el token que generamos → Abilita el acceso al notebook.\n",
    "3. Ejecutar la celda siguiente para leerlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1756393847450,
     "user": {
      "displayName": "Joaquin Bonifacino",
      "userId": "00057501700847538249"
     },
     "user_tz": 180
    },
    "id": "DMTYDzh9rB0A",
    "outputId": "e3816a08-48ac-4f2a-848b-fcfbc6f39ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token cargado OK: {HF_TOKEN}\n"
     ]
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "assert HF_TOKEN is not None and len(HF_TOKEN) > 0, \"Configurar el secreto 'HF_TOKEN' en Colab.\"\n",
    "print(\"Token cargado OK: {HF_TOKEN}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQbt6t3FrB0A"
   },
   "source": [
    "## Parte 1 — Uso básico de LLMs con LangChain\n",
    "\n",
    "Trabajaremos con un modelo instruct accesible vía Inference API. Para minimizar fricción, usaremos un modelo abierto y, cuando sea posible, una variante quantizada (el ya hosteado en HF).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1458,
     "status": "ok",
     "timestamp": 1756393944261,
     "user": {
      "displayName": "Joaquin Bonifacino",
      "userId": "00057501700847538249"
     },
     "user_tz": 180
    },
    "id": "fNzk__gZrB0B",
    "outputId": "abb8b0bf-ab04-4a36-adda-a863f3c4ffd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un LLM (Large Language Model) es un tipo de modelo de inteligencia artificial entrenado en grandes volúmenes de texto para entender, generar y realizar tareas relacionadas con el lenguaje humano.  \n",
      "Casos de uso incluyen la redacción de contenido automatizada y asistencia en resolución de preguntas.  \n",
      "Estos modelos también se utilizan en chatbots, traducción automática y generación de código.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "hf_endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=MODEL_ID,\n",
    "    task=\"conversational\",\n",
    "    huggingfacehub_api_token=\"HF_TOKEN\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "llm = ChatHuggingFace(llm=hf_endpoint)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente útil y conciso.\"),\n",
    "    (\"human\", \"Responde a la siguiente instrucción: {instruccion}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\"instruccion\": \"Explica en 3 frases qué es un LLM y nombra 2 casos de uso.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM0fs9ZLrB0B"
   },
   "source": [
    "### Ejercicio 1.1 (10 min)\n",
    "\n",
    "- Probar 3 variaciones de `temperature` y observar el cambio en estilo.\n",
    "- Cambiar el rol del `system` para forzar un estilo (p.ej., “responde con viñetas y máximo 3 líneas”).\n",
    "- Pregunta sugerida: “Resume la diferencia entre entrenamiento y fine-tuning en 3 puntos.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mcTYMM-srB0B",
    "outputId": "22adbb5a-1be3-4655-caec-b27466e2e669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== temperature: 0.0 ====\n",
      " 1. **Entrenamiento (training)**: Se refiere al proceso inicial de entrenar un modelo desde cero o a partir de un modelo preentrenado en grandes volúmenes de datos generales, para que aprenda características básicas de un dominio o tarea específica.\n",
      "\n",
      "2. **Fine-tuning**: Consiste en ajustar un modelo ya entrenado (normalmente en un dominio general) con datos más específicos y relevantes para una tarea o dominio particular, para mejorar su rendimiento en esa tarea específica.\n",
      "\n",
      "3. **Objetivo**: El entrenamiento busca aprender representaciones generales, mientras que el fine-tuning busca optimizar el modelo para una aplicación concreta, usando menos recursos y tiempo que un entrenamiento desde cero.\n",
      "\n",
      "==== temperature: 0.7 ====\n",
      " 1. **Entrenamiento (training)**: Consiste en entrenar un modelo desde cero o a partir de un modelo inicial, utilizando un conjunto de datos amplio y diverso para aprender patrones generales de una tarea o dominio.\n",
      "\n",
      "2. **Fine-tuning**: Se realiza después de un entrenamiento inicial, utilizando un conjunto de datos más específico para ajustar el modelo a una tarea o domino particulares, lo que mejora su rendimiento en ese contexto.\n",
      "\n",
      "3. **Objetivo**: El entrenamiento busca aprender habilidades generales, mientras que el fine-tuning busca optimizar el modelo para una aplicación específica, haciendo que sea más preciso y adecuado.\n",
      "\n",
      "==== temperature: 1.2 ====\n",
      " 1. **Entrenamiento (training)**: Se realiza desde cero o con un modelo inicial, usando un conjunto de datos amplio y diverso para aprender patrones generales de datos, como lenguaje, imágenes o números.  \n",
      "\n",
      "2. **Fine-tuning**: Se aplica después de un entrenamiento inicial, ajustando el modelo con un conjunto de datos más específico y pequeño para adaptarlo a una tarea o dominio particular (por ejemplo, clasificación de noticias o traducción).  \n",
      "\n",
      "3. **Objetivo**: El entrenamiento busca aprender representaciones generales; el fine-tuning busca optimizar el modelo para una tarea específica, mejorando su precisión y rendimiento en entornos reales.\n",
      "\n",
      "==== estilo forzado ====:\n",
      " - Entrenamiento: se entrena un modelo desde cero o a partir de un modelo inicial con datos genéricos.  \n",
      "- Fine-tuning: se adapta un modelo ya entrenado a un dominio específico usando datos relevantes.  \n",
      "- Fine-tuning requiere menos recursos que el entrenamiento desde cero y es más eficiente para tareas específicas.\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 1.1 — Variaciones de temperatura y estilo (compatible con nscale)\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "instruccion: str = \"Resume la diferencia entre entrenamiento y fine-tuning en 3 puntos.\"\n",
    "\n",
    "# Variar temperatura (crea un endpoint conversacional por cada T)\n",
    "for t in [0.0, 0.7, 1.2]:\n",
    "    tmp_endpoint = HuggingFaceEndpoint(\n",
    "        repo_id=MODEL_ID,\n",
    "        task=\"conversational\",\n",
    "        huggingfacehub_api_token=\"HF_TOKEN\",\n",
    "        temperature=t,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "    tmp_llm = ChatHuggingFace(llm=tmp_endpoint)\n",
    "    respuesta = (prompt | tmp_llm | StrOutputParser()).invoke({\"instruccion\": instruccion})\n",
    "    print(\"\\n==== temperature:\", t, \"====\\n\", respuesta)\n",
    "\n",
    "# Forzar estilo con system prompt (usa un LLM base conversacional)\n",
    "base_endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=MODEL_ID,\n",
    "    task=\"conversational\",\n",
    "    huggingfacehub_api_token=HF_TOKEN,\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "llm_base = ChatHuggingFace(llm=base_endpoint)\n",
    "\n",
    "prompt_estilo = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres conciso. Responde SIEMPRE con viñetas y máximo 3 líneas.\"),\n",
    "    (\"human\", \"{instruccion}\"),\n",
    "])\n",
    "\n",
    "respuesta_estilo = (prompt_estilo | llm_base | StrOutputParser()).invoke({\"instruccion\": instruccion})\n",
    "print(\"\\n==== estilo forzado ====:\\n\", respuesta_estilo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mi_Cg8IMrB0C"
   },
   "source": [
    "## Parte 1.2 — Parámetros de decodificación\n",
    "\n",
    "Ajustaremos parámetros como `top_p`, `repetition_penalty` y `max_new_tokens` para observar su efecto en la generación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6lCnXq_QrB0C",
    "outputId": "f1c41891-4547-4ab4-ea39-d780948d4125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== baseline ({'temperature': 0.7, 'top_p': 0.95, 'repetition_penalty': 1.0, 'max_new_tokens': 128}) ====\n",
      "¡Claro! Aquí tienes una analogía breve y sencilla:\n",
      "\n",
      "**RAG es como si tuvieras un libro enorme con información actualizada, y en lugar de memorizarlo todo, cada vez que necesitas una respuesta, buscas solo los párrafos más relevantes que te ayuden a responder con algo útil y preciso.**\n",
      "\n",
      "Así, no se trata de recordar todo, sino de buscar la información necesaria en tiempo real.\n",
      "\n",
      "==== creativo ({'temperature': 1.1, 'top_p': 0.9, 'repetition_penalty': 1.0, 'max_new_tokens': 128}) ====\n",
      "Imagina que tienes un gran libro con millones de páginas, y necesitas responder una pregunta. En lugar de leer todo el libro, solo buscas las páginas más relevantes que tengan información sobre tu pregunta. El RAG (Retrieval-Augmented Generation) es como un sistema que busca rápidamente las páginas más útiles en un libro enorme (los datos) y luego las usa para crear una respuesta clara y precisa. Así, se combina inteligencia del conocimiento existente con la capacidad de generar respuestas.\n",
      "\n",
      "==== controlado ({'temperature': 0.2, 'top_p': 0.8, 'repetition_penalty': 1.1, 'max_new_tokens': 96}) ====\n",
      "Imagina que tienes un gran libro de recetas, y quieres cocinar un plato que no has visto nunca. En lugar de abrir el libro y buscar directamente la receta, decides preguntarle a un chef que conoce bien los ingredientes. Él te responde con información directa, basada en lo que ha aprendido y en lo que sabes. Así, RAG (Retrieval-Augmented Generation) es como ese chef: no inventa la receta, sino que busca información útil en una gran cantidad de datos para ayudar a crear una respuesta precisa y relevante.\n"
     ]
    }
   ],
   "source": [
    "# Exploración de parámetros de decodificación\n",
    "from typing import Dict, Any\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "consulta: str = \"Escribe una analogía breve para explicar RAG a un público no técnico.\"\n",
    "\n",
    "configuraciones: Dict[str, Dict[str, Any]] = {\n",
    "    \"baseline\":   {\"temperature\": 0.7, \"top_p\": 0.95, \"repetition_penalty\": 1.0, \"max_new_tokens\": 128},\n",
    "    \"creativo\":   {\"temperature\": 1.1, \"top_p\": 0.90, \"repetition_penalty\": 1.0, \"max_new_tokens\": 128},\n",
    "    \"controlado\": {\"temperature\": 0.2, \"top_p\": 0.80, \"repetition_penalty\": 1.1, \"max_new_tokens\": 96},\n",
    "}\n",
    "\n",
    "for nombre, cfg in configuraciones.items():\n",
    "    tmp_endpoint = HuggingFaceEndpoint(\n",
    "        repo_id=MODEL_ID,\n",
    "        task=\"conversational\",\n",
    "        huggingfacehub_api_token=HF_TOKEN,\n",
    "        temperature=cfg[\"temperature\"],\n",
    "        top_p=cfg[\"top_p\"],\n",
    "        repetition_penalty=cfg[\"repetition_penalty\"],\n",
    "        max_new_tokens=cfg[\"max_new_tokens\"],\n",
    "    )\n",
    "    tmp_llm = ChatHuggingFace(llm=tmp_endpoint)\n",
    "    salida = (prompt | tmp_llm | StrOutputParser()).invoke({\"instruccion\": consulta})\n",
    "    print(f\"\\n==== {nombre} ({cfg}) ====\\n{salida}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFWQ7vryrB0C"
   },
   "source": [
    "## Parte 2 — Prompt engineering avanzado\n",
    "\n",
    "Exploraremos estrategias para mejorar la calidad y control de las respuestas: zero-shot, few-shot, restricciones de estilo, salida estructurada, Chain of Thought y Self-Consistency/Ensemble.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1U5MiS1rB0C"
   },
   "source": [
    "### Conceptos clave: Zero-shot, Few-shot, CoT, Roles y Self-Consistency\n",
    "\n",
    "- Zero-shot: el modelo resuelve la tarea solo con instrucciones; no se proporcionan ejemplos.\n",
    "- Few-shot: además de la instrucción, se incluyen 1-3 ejemplos que muestran el formato y estilo deseados.\n",
    "- Chain-of-Thought (CoT): se guía al modelo para mostrar pasos intermedios de razonamiento (p.ej., “razona paso a paso”) antes de una respuesta final.\n",
    "- Role prompting: se asigna un rol (p.ej., “actúa como profesor de IA”) para influir en el estilo y nivel de detalle.\n",
    "- Self-Consistency / Ensemble: se generan múltiples respuestas estocásticas (con temperatura > 0) y se vota/selecciona la respuesta más consistente o frecuente.\n",
    "- JSON output: se pide una salida estricta en formato JSON y se valida con un parser.\n",
    "\n",
    "Lectura recomendada: guía de prompt engineering avanzada en `https://learnprompting.org/docs/introduction`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiF9jJRfrB0C"
   },
   "source": [
    "### Zero-shot y Few-shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fNZ9KnrLrB0D",
    "outputId": "0d87f5fd-e84b-4da8-d926-d12e9494575f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ZERO-SHOT ===\n",
      "Overfitting occurs when a model learns noise instead of patterns.  \n",
      "Overcomplicated models fail on new, unseen data.  \n",
      "Overfitting reduces generalization in predictions.\n",
      "\n",
      "=== FEW-SHOT ===\n",
      "O Viola la generalización al ajustarse demasiado a datos de entrenamiento  \n",
      "V Evalúa mal en datos nuevos por falta de robustez  \n",
      "E Excede el poder de representación del modelo\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot vs Few-shot (diferencia marcada con patrón acróstico)\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM mas determinista\n",
    "det_endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=MODEL_ID, task=\"conversational\",\n",
    "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=128\n",
    ")\n",
    "llm_det = ChatHuggingFace(llm=det_endpoint)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Patrón no obvio: acróstico O-V-E (Overfitting), cada línea empieza con esa letra\n",
    "instruccion = (\n",
    "    \"Escribe EXACTAMENTE 3 líneas sobre 'overfitting'. \"\n",
    "    \"Cada línea debe comenzar con O, luego V, luego E (en ese orden). \"\n",
    "    \"6-10 palabras por línea. Sin texto extra.\"\n",
    ")\n",
    "\n",
    "zero_shot = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Sigue estrictamente las instrucciones del usuario.\"),\n",
    "    (\"human\", \"{instruccion}\")\n",
    "])\n",
    "\n",
    "few_shot = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Sigue estrictamente las instrucciones del usuario.\"),\n",
    "    # Ejemplo: el patrón se demuestra con otro tema y otro acróstico (R-E-G)\n",
    "    (\"human\", \"Escribe EXACTAMENTE 3 líneas sobre 'regularización'. \"\n",
    "              \"Cada línea debe comenzar con R, luego E, luego G. \"\n",
    "              \"6-10 palabras por línea. Sin texto extra.\"),\n",
    "    (\"ai\", \"R Reduce complejidad para evitar ajustes al ruido\\n\"\n",
    "           \"E Estabiliza el aprendizaje con penalizaciones adecuadas\\n\"\n",
    "           \"G Generaliza mejor limitando pesos excesivamente grandes\"),\n",
    "    # Ahora se pide el caso real con el acróstico O-V-E\n",
    "    (\"human\", \"{instruccion}\")\n",
    "])\n",
    "\n",
    "print(\"=== ZERO-SHOT ===\")\n",
    "print((zero_shot | llm_det | parser).invoke({\"instruccion\": instruccion}))\n",
    "\n",
    "print(\"\\n=== FEW-SHOT ===\")\n",
    "print((few_shot | llm_det | parser).invoke({\"instruccion\": instruccion}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b7hTF4wrB0D"
   },
   "source": [
    "### Role prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_VhEgQZrB0D",
    "outputId": "fd5e846d-cafc-4dff-fb4f-f762db9e9cd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, aquí tienes una explicación breve y clara:\n",
      "\n",
      "**Qué es el aprendizaje por refuerzo (Reinforcement Learning):**  \n",
      "Es un tipo de inteligencia artificial donde un agente (como un robot o un programa) aprende a tomar decisiones para maximizar una recompensa a lo largo del tiempo. No tiene acceso a datos etiquetados, sino que aprende probando diferentes acciones y viéndose recompensado o sancionado según los resultados.\n",
      "\n",
      "🔍 **Por ejemplo:**  \n",
      "Imagina que un agente quiere aprender a jugar al ajedrez. Cada vez que gana, recibe una recompensa. Si pierde, la recompensa es baja o negativa. Con el tiempo, el agente aprende qué movimientos le llevan a ganar más.\n",
      "\n",
      "### 2 ejemplos de aplicación:\n",
      "1. **Juegos de video** (como AlphaGo o Deep Q-Networks): Los algoritmos aprenden a jugar mejor jugando miles de partidas y recibiendo recompensas por ganar.\n",
      "2. **Robótica y automatización** (como un robot que aprende a caminar o navegar): El robot recibe recompensas cuando se mueve bien o evita caídas, y ajusta su comportamiento para mejorar.\n",
      "\n",
      "En resumen: **El aprendizaje por refuerzo es aprender por intentos, errores y recompensas.**\n"
     ]
    }
   ],
   "source": [
    "# Role prompting\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "role_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Actúa como profesor de IA de nivel intermedio. Sé claro, estructurado y usa ejemplos sencillos.\"),\n",
    "    (\"human\", \"Explica brevemente qué es el aprendizaje por refuerzo y menciona 2 ejemplos de aplicación.\"),\n",
    "])\n",
    "\n",
    "print((role_prompt | llm | StrOutputParser()).invoke({}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFhoZDI4rB0D"
   },
   "source": [
    "### CoT (Chain-of-Thought) prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AK2aANwrB0D",
    "outputId": "2e40d0fd-99ee-4a28-943a-63dc34811eeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIN CoT ===\n",
      "9.09%\n",
      "\n",
      "=== CON CoT ===\n",
      "Vamos a resolver este problema paso a paso usando el **teorema de Bayes**.\n",
      "\n",
      "---\n",
      "\n",
      "### Datos del problema:\n",
      "\n",
      "- Probabilidad de tener la enfermedad (prevalencia):  \n",
      "  \\( P(E) = 1\\% = 0.01 \\)\n",
      "\n",
      "- Probabilidad de no tener la enfermedad:  \n",
      "  \\( P(\\neg E) = 1 - 0.01 = 0.99 \\)\n",
      "\n",
      "- Sensibilidad de la prueba (probabilidad de dar positivo si tienes la enfermedad):  \n",
      "  \\( P(T^+ | E) = 90\\% = 0.90 \\)\n",
      "\n",
      "- Especificidad de la prueba (probabilidad de dar negativo si no tienes la enfermedad):  \n",
      "  \\( P(T^- | \\neg E) = 90\\% = 0.90 \\)\n",
      "\n",
      "Esto implica que:\n",
      "\n",
      "- El error de falsa positiva es \\( P(T^+ | \\neg E) = 1 - 0.90 = 0.10 \\)\n",
      "\n",
      "Nos piden:  \n",
      "**Probabilidad de que una persona realmente tenga la enfermedad dado que dio positivo.**  \n",
      "Es decir:  \n",
      "\\( P(E | T^+) = ? \\)\n",
      "\n",
      "---\n",
      "\n",
      "### Usamos el teorema de Bayes:\n",
      "\n",
      "\\[\n",
      "P(E | T^+) = \\frac{P(T^+ | E) \\cdot P(E)}{P(T^+)}\n",
      "\\]\n",
      "\n",
      "Donde \\( P(T^+) \\) es la probabilidad total de dar positivo. Para calcularlo, usamos el **teorema de la probabilidad total**:\n",
      "\n",
      "\\[\n",
      "P(T^+) = P(T^+ | E) \\cdot P(E) + P(T^+ | \\neg E) \\cdot P(\\neg E)\n",
      "\\]\n",
      "\n",
      "Sustituimos los valores:\n",
      "\n",
      "\\[\n",
      "P(T^+) = (0.90)(0.01) + (0.10)(0.99) = 0.009 + 0.099 = 0.108\n",
      "\\]\n",
      "\n",
      "Ahora aplicamos Bayes:\n",
      "\n",
      "\\[\n",
      "P(E | T^+) = \\frac{0.90 \\times 0.01}{0.108} = \\frac{0.009}{0.108} \\approx 0.0833\n",
      "\\]\n",
      "\n",
      "Convertimos a porcentaje:\n",
      "\n",
      "\\[\n",
      "0.0833 \\times 100 \\approx 8.33\\%\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "Respuesta final: 8.33%\n"
     ]
    }
   ],
   "source": [
    "# Prompts SIN CoT y CON CoT (Bayes) — nscale-compatible\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "problema = (\n",
    "    \"En una población, 1% tiene la enfermedad. La prueba tiene 90% de sensibilidad y 90% de especificidad. \"\n",
    "    \"Si una persona da positivo, ¿cuál es la probabilidad (en %) de que realmente esté enferma?\"\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# SIN CoT: SOLO porcentaje en una línea (recorta tokens)\n",
    "endpoint_sin = HuggingFaceEndpoint(\n",
    "    repo_id=MODEL_ID, task=\"conversational\",\n",
    "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=6\n",
    ")\n",
    "llm_sin = ChatHuggingFace(llm=endpoint_sin)\n",
    "\n",
    "prompt_sin_cot = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Devuelve SOLO un número en formato porcentaje (ej: 8.33%). \"\n",
    "               \"Sin explicaciones, sin ecuaciones, sin texto extra.\"),\n",
    "    (\"human\", \"{q}\")\n",
    "])\n",
    "\n",
    "# CON CoT: piensa paso a paso y cierra con una línea final\n",
    "endpoint_con = HuggingFaceEndpoint(\n",
    "    repo_id=MODEL_ID, task=\"conversational\",\n",
    "    huggingfacehub_api_token=HF_TOKEN, temperature=0.0, max_new_tokens=256\n",
    ")\n",
    "llm_con = ChatHuggingFace(llm=endpoint_con)\n",
    "\n",
    "prompt_con_cot = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tómate tu tiempo y piensa paso a paso usando Bayes. \"\n",
    "               \"Al final, da una sola línea con: 'Respuesta final: <n>%'\"),\n",
    "    (\"human\", \"{q}\")\n",
    "])\n",
    "\n",
    "print(\"=== SIN CoT ===\")\n",
    "print((prompt_sin_cot | llm_sin | parser).invoke({\"q\": problema}))\n",
    "\n",
    "print(\"\\n=== CON CoT ===\")\n",
    "print((prompt_con_cot | llm_con | parser).invoke({\"q\": problema}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXteBwdorB0F"
   },
   "source": [
    "### Self-Consistency / Ensemble prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "7jpZCrBqrB0F",
    "outputId": "94ba1088-4758-41dd-90b8-4f448bd831c5"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1626907301.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     \u001b[0mtmp_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatHuggingFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_endpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtexto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mtmp_llm\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"instruccion\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpregunta\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mrespuestas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3047\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3048\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3049\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3050\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3051\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m         return cast(\n\u001b[1;32m    382\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1005\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m                 results.append(\n\u001b[0;32m--> 825\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    826\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1073\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_huggingface/chat_models/huggingface.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             }\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mllm_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_chat_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         )\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36m_inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_as_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_as_binary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m                 response = get_session().post(\n\u001b[0m\u001b[1;32m    265\u001b[0m                     \u001b[0mrequest_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                     \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Send: {_curlify(request)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_AMZN_TRACE_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ejemplo: self-consistency simple (compatible con nscale/conversational)\n",
    "from collections import Counter\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "pregunta = \"Lista 3 riesgos comunes al usar LLMs en producción.\"\n",
    "respuestas = []\n",
    "parser = StrOutputParser()\n",
    "\n",
    "for _ in range(5):\n",
    "    tmp_endpoint = HuggingFaceEndpoint(\n",
    "        repo_id=MODEL_ID,\n",
    "        task=\"conversational\",\n",
    "        huggingfacehub_api_token=HF_TOKEN,\n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=200,\n",
    "    )\n",
    "    tmp_llm = ChatHuggingFace(llm=tmp_endpoint)\n",
    "    texto = (prompt | tmp_llm | parser).invoke({\"instruccion\": pregunta})\n",
    "    respuestas.append(texto.strip())\n",
    "\n",
    "conteo = Counter(respuestas)\n",
    "mejor_respuesta, _ = conteo.most_common(1)[0]\n",
    "\n",
    "print(\"\\n==== Candidatas ====\")\n",
    "for i, r in enumerate(respuestas, 1):\n",
    "    print(f\"[{i}]\\n{r}\\n\")\n",
    "\n",
    "print(\"==== Seleccionada por frecuencia ====\")\n",
    "print(mejor_respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SPYi9X1rB0F"
   },
   "source": [
    "### Salida estructurada (JSON Output Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCegKKRDrB0F",
    "outputId": "0a6642d1-926b-4683-8e6a-0da447fc7e05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"titulo\": \"RAG (Retrieval-Augmented Generation)\",\n",
      "  \"puntos_clave\": [\n",
      "    \"RAG combina la recuperación de información de una base de conocimientos con la generación de texto para mejorar la precisión y relevancia.\",\n",
      "    \"Permite que los modelos de lenguaje utilicen información de documentos externos en tiempo real durante la generación de respuestas.\",\n",
      "    \"Reduce el riesgo de generar contenido fabricado al basarse en fuentes verificadas de información.\"\n",
      "  ],\n",
      "  \"dificultad\": \"intermedio\"\n",
      "}\n",
      "JSON válido con las claves requeridas.\n"
     ]
    }
   ],
   "source": [
    "# Salida estructurada (JSON) con output parser\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "json_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Eres un asistente que devuelve SIEMPRE JSON válido. Dado un tema, devuelve un objeto con:\n",
    "    - \"titulo\": string\n",
    "    - \"puntos_clave\": lista de 3 strings\n",
    "    - \"dificultad\": uno de [\"básico\", \"intermedio\", \"avanzado\"]\n",
    "    Responde SOLO con JSON válido sin texto adicional.\n",
    "    Tema: {tema}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "json_text = (json_prompt | llm | StrOutputParser()).invoke({\"tema\": \"RAG\"})\n",
    "print(json_text)\n",
    "\n",
    "data = json.loads(json_text)\n",
    "assert set([\"titulo\", \"puntos_clave\", \"dificultad\"]).issubset(data.keys())\n",
    "print(\"JSON válido con las claves requeridas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC4DngMVrB0G"
   },
   "source": [
    "## Ejercicios — Parte 2\n",
    "\n",
    "Resuelve los siguientes ejercicios. Modifica prompts y parámetros si es necesario y justifica brevemente tus decisiones (en una celda de texto).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFAmf9D0rB0G"
   },
   "source": [
    "### Ejercicio 2.1 — Zero-shot vs Few-shot\n",
    "\n",
    "- Tarea: explicar “regularización L2” en 3 viñetas claras para un público técnico.\n",
    "- Paso 1 (zero-shot): crea un prompt sin ejemplos y observa el resultado.\n",
    "- Paso 2 (few-shot): agrega 1-2 ejemplos de estilo y compara la salida.\n",
    "- Pregunta guía: ¿mejoró la precisión o claridad con pocos ejemplos? Justifica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fd2vCaKlrB0G",
    "outputId": "9b6ac954-82d1-49b7-adb9-1794ee3e248c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ZERO-SHOT ===\n",
      "\n",
      "- La **regularización L2** añade un término al costo de pérdida (loss function) que penaliza el tamaño de los coeficientes del modelo, específicamente sumando el cuadrado de todos los pesos, multiplicado por un parámetro de regularización λ.  \n",
      "- Este enfoque tiende a **reducir el overfitting** al promover soluciones con coeficientes más pequeños y distribuidos, lo que implica una modelación más generalizable.  \n",
      "- En términos matemáticos, si la función de pérdida original es $ \\mathcal{L} $, la versión regularizada se expresa como $ \\mathcal{L} + \\lambda \\sum_{i} w_i^2 $, donde $ w_i $ son los coeficientes del modelo y $ \\lambda $ controla el grado de penalización.\n",
      "\n",
      "=== FEW-SHOT ===\n",
      "\n",
      "- Penaliza cuadrado de magnitud de coeficientes  \n",
      "- Promueve coeficientes pequeños pero no nulos  \n",
      "- Ayuda a evitar sobreajuste en modelos de regresión\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 2.1 — Zero-shot vs Few-shot (código)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Zero-shot\n",
    "zero_shot_A = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente técnico y claro. Responde en viñetas.\"),\n",
    "    (\"human\", \"Explica 'regularización L2' en 3 viñetas para un público técnico.\"),\n",
    "])\n",
    "print(\"\\n=== ZERO-SHOT ===\\n\")\n",
    "print((zero_shot_A | llm | StrOutputParser()).invoke({}))\n",
    "\n",
    "# Few-shot (agregar 1-2 ejemplos de estilo)\n",
    "few_shot_A = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente técnico y claro. Responde en viñetas.\"),\n",
    "    (\"human\", \"Explica 'regularización L1' en 3 viñetas\"),\n",
    "    (\"ai\", \"- Penaliza magnitud absoluta\\n- Induce sparsidad en coeficientes\\n- Útil para selección de variables\"),\n",
    "    (\"human\", \"Explica 'regularización L2' en 3 viñetas\"),\n",
    "])\n",
    "print(\"\\n=== FEW-SHOT ===\\n\")\n",
    "print((few_shot_A | llm | StrOutputParser()).invoke({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsrH2ZdtrB0G"
   },
   "source": [
    "### Ejercicio 2.2 — Chain-of-Thought (CoT)\n",
    "\n",
    "- Tarea: dado un problema de evaluación de modelos, razonar paso a paso y entregar una conclusión final breve.\n",
    "- Problema sugerido: “¿Por qué accuracy puede ser engañoso en un dataset muy desbalanceado y qué métrica alternativa usarías?”\n",
    "- Pista: pide explícitamente “razona paso a paso y luego da una respuesta final breve en una línea”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37-CDC6irB0G",
    "outputId": "a73ad459-a554-478d-a8b4-2aa2b46d9959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 1: En un dataset desbalanceado, la mayoría de las instancias pertenecen a una clase, por lo que un modelo puede predecir solo esa clase con alta frecuencia y obtener una accuracy alta, aunque sea inútil prácticamente.\n",
      "\n",
      "Paso 2: Esta alta accuracy es engañosa porque no refleja el desempeño real en las clases minoritarias, que suelen ser más importantes en aplicaciones reales.\n",
      "\n",
      "Paso 3: Una métrica alternativa que considera mejor el desempeño en todas las clases es la precisión (precision), el recall (o tasa de detección), o el F1-score, especialmente el F1-score, que es el promedio armonicamente entre precisión y recall.\n",
      "\n",
      "Paso 4: El F1-score ofrece un balance adecuado entre precisión y recall, y es más robusto en datos desbalanceados.\n",
      "\n",
      "Respuesta final: La accuracy puede ser engañoso en datasets desbalanceados porque favorece la clase mayoritaria; se recomienda usar el F1-score como métrica alternativa.\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 2.2 — CoT (código)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "cot_ex = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un tutor de IA. Razona paso a paso y luego da una respuesta final breve.\"),\n",
    "    (\"human\", \"Problema: {problema}\\nIndica tus pasos de razonamiento y luego una respuesta final en una sola línea.\"),\n",
    "])\n",
    "\n",
    "problema = \"¿Por qué accuracy puede ser engañoso en un dataset muy desbalanceado y qué métrica alternativa usarías?\"\n",
    "print((cot_ex | llm | StrOutputParser()).invoke({\"problema\": problema}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRDMh9ewrB0G"
   },
   "source": [
    "### Ejercicio 2.3 — Role prompting\n",
    "\n",
    "- Tarea: explicar el “sesgo de selección” a un equipo de data engineering con ejemplos concisos.\n",
    "- Rol: “Actúa como líder técnico de datos; sé pragmático y directo, con viñetas concretas”.\n",
    "- Objetivo: evaluar cómo cambia el estilo bajo un rol técnico específico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnGf_iFWrB0G",
    "outputId": "d8d72898-8cd6-4c23-9519-36f245d590bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **El sesgo de selección ocurre cuando los datos usados para entrenar un modelo o tomar decisiones no representan adecuadamente a toda la población objetivo**, lo que lleva a predicciones o conclusiones sesgadas.  \n",
      "- **En data engineering, esto suele manifestarse en la forma en que se recolectan, filtran o procesan los datos**: por ejemplo, solo usar registros de usuarios activos en una aplicación, ignorando a los inactivos.  \n",
      "- **El riesgo es que los modelos se entrenen en subconjuntos distorsionados**, resultando en decisiones que no se generalizan bien (como un algoritmo de recomendación que nunca sugiere contenidos para usuarios pasivos).\n",
      "\n",
      "**Ejemplo práctico**:  \n",
      "Un equipo de datos construye un sistema de detección de fraudes que solo se entrena con transacciones de usuarios de cierto país (por ejemplo, EE.UU.). Como el perfil de transacciones y comportamientos de fraudes varían en otros países, el modelo falla al detectar fraudes en transacciones de usuarios de España, por lo que **el sesgo de selección distorsiona la capacidad del sistema para generalizarse**.\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 2.3 — Role prompting (código)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "role_ex = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Actúa como líder técnico de datos; sé pragmático y directo, con viñetas concretas.\"),\n",
    "    (\"human\", \"Explica el ‘sesgo de selección’ a un equipo de data engineering en 3 viñetas y da 1 ejemplo práctico.\"),\n",
    "])\n",
    "\n",
    "print((role_ex | llm | StrOutputParser()).invoke({}))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
